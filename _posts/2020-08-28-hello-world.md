---
layout: post
title: Hello World
author: Chengyi Wang
date: 2020-08-28 10:42:53 +0800
tags: [test, hello]
---

Hello World! This is a post for testing.

Attention-based sequence-to-sequence models [1, 18, 19, 4, 12, 7], especially transformer model [18], have shown great success in various tasks, e.g. neural machine translation [1, 18], image captioning [19] and speech recognition [4, 12, 7].

For conventional attention-based sequence-to-sequence models, the inference process can be divided into two stages. The encoder first processes an entire input sequence into a high-level state sequence. After that, the decoder predicts the output sequence conditioned on the previous predicted symbol and context vector extracted from the entire encoded state sequence. This makes the models encode and decode sequences asynchronously, and prevents it from being applied for online speech recognition. There are some works trying to release this problem. Tjandra et al. [17] propose a local monotonic attention mechanism that forces the model to predict a central position at every decoding step and calculate soft attention weights only around the central position. However, itâ€™s difficult to accurately predict the next central position just based on limited information. Monotonic chunkwise attention [3] is proposed to adaptively split the encoded state sequence into small chunks based on the predicted selection probabilities. But complex and tricky training methods make it hard to implement. Triggered attention [13] utilizes the spikes produced by connectionist temporal classification (CTC) model to split the sequence into many state chunks, and then the decoder predicts the output sequence in a chunkwise way. However, triggered attention requires forced alignment to assist model training. Most of the proposed models introduce additional components and have very tricky training methods.

In this paper, we propose a synchronous transformer model (Sync-Transformer), which can perform encoding and decoding at the same time. The Sync-Transformer combines the transformer [7] and self-attention transducer (SA-T) [16] in great depth. Similar to the original transformer, the Sync-Transformer has an encoder and a decoder. In order to eliminate the dependencies of self-attention mechanism on the future information, we first force every node in the encoder to only focus on its left contexts and ignore its right contexts completely. Once a fixed-length chunk of state sequence is produced by the encoder, the decoder begins to predict symbols immediately. Similar to the Neural Transducer [11, 10, 14], the decoder generates the output sequence chunk by chunk. However, restricted by the time-dependent property of RNNs, the Neural Transducer model only optimizes the approximate best alignment path corresponding to the chunk sequence. By contrast, we adopt a forward-backward algorithm to optimize all possible alignment paths and calculate the negative log loss function as the same as the RNN-Transducer [8] and SA-T [16]. We evaluate our Sync-Transformer on a Mandarin dataset AISHELL-1. The experiments show that the Sync-Transformer is able to encode and decode sequences synchronously and achieves a comparable performance with the transformer.

The remainder of this paper is organized as follows. Section 2 describes our proposed Sync-Transformer. Section 3 presents our experimental setup and results. The conclusions and future work will be given in Section 4.
